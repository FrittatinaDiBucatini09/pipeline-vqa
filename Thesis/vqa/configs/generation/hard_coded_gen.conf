# ==============================================================================
# CONFIGURATION FILE: Hardcoded Default Parameters
# ==============================================================================
# This file contains all explicit default values as defined in the main
# run_generation.sh script.
# ==============================================================================


# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
MODEL_NAME="google/medgemma-4b-it"  # Model to use for VQA inference

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
# Option 1: HuggingFace Dataset
DATASET_NAME=""                                 # HuggingFace dataset name (leave empty to use local file)
SPLIT="train"                                   # Dataset split (train/test/validation)

# Option 2: Local File
DATA_FILE="gemex_VQA_mimic_mapped.csv"          # Path to local dataset file (JSON or CSV)

# Dataset Column Names
IMAGE_COLUMN="image_path"                       # Name of image column in dataset
QUESTION_COLUMN="question"                      # Name of question column in dataset
ANSWER_COLUMN="answer"                          # Name of answer column in dataset

# Output Configuration
OUTPUT_DIR="results/vqa_results"                # Directory to save results
IMAGES_DIR="images"                             # Directory to save images (for HuggingFace datasets)
MAX_SAMPLES=""                                  # Maximum samples to evaluate (empty = all)

# ============================================================================
# INFERENCE CONFIGURATION
# ============================================================================
BATCH_SIZE=16                            # Batch size for inference
MAX_TOKENS=100                           # Maximum tokens to generate
TEMPERATURE=0.0                          # Sampling temperature (0.0 = greedy)
TOP_K=-1                                 # Top-k sampling (-1 = disabled)
TOP_P=1.0                                # Top-p sampling (1.0 = disabled)
MIN_P=0.0                                # Min-p sampling (0.0 = disabled)
GPU_MEMORY_UTILIZATION=0.6               # Fraction of GPU memory for vLLM (0.0-1.0)

# ============================================================================
# PROMPTING STRATEGY
# ============================================================================
USE_COT=false                            # Enable Chain-of-Thought prompting
USE_IMAGES=true                          # Enable multimodal input with images
ENABLE_THINKING=false                    # Enable thinking mode

# ============================================================================
# FEW-SHOT LEARNING CONFIGURATION
# ============================================================================
USE_FEW_SHOT=true                       # Enable few-shot prompting
NUM_FEW_SHOT=5                          # Number of few-shot examples
FEW_SHOT_SEED=42                        # Random seed for few-shot sampling

# ============================================================================
# OUTPUT CONTROL
# ============================================================================
SAVE_GENERATIONS=true                   # Save model generations to file