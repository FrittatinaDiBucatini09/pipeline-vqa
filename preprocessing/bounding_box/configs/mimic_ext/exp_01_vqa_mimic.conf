# ==============================================================================
# EXPERIMENT E5: MIMIC-Ext-CXR-VQA (Text-Only Inference Pipeline)
# ==============================================================================
# Description:
#   Performs VQA inference on the MIMIC-Ext dataset (text-only annotations).
#   Utilizes natural language questions from the dataset for guided localization.
#   NOTE: Without ground-truth boxes, only generates predictions (no evaluation metrics).
# ==============================================================================

# ------------------------------------------------------------------------------
# 1. DATASET CONFIGURATION & COLUMN MAPPING
# ------------------------------------------------------------------------------
# Ensure the CSV file is in the project root (same directory as bbox_preprocessing.py)
METADATA_FILENAME="mimic_ext_mapped.csv" 
PATH_COLUMN="image_path"
# CRITICAL: TEXT_COLUMN determines prompt source - uses dataset questions
TEXT_COLUMN="question"     
BOX_COLUMN="visual_locations"  # Placeholder field (empty "[]") - required for schema compatibility

# ------------------------------------------------------------------------------
# 2. PROMPTING STRATEGY
# ------------------------------------------------------------------------------
# Core: Use dataset's 'question' column instead of static prompts
USE_DYNAMIC_PROMPTS="true"   
# Disable visual regions (not available in MIMIC-Ext annotations)
USE_VISUAL_REGIONS="false"   
COMPOSITE_REGIONS="false"
EXPLODE_REGIONS="false"

# ------------------------------------------------------------------------------
# 3. INFERENCE PARAMETERS
# ------------------------------------------------------------------------------
# Must be INFERENCE mode (Gold mode would produce no output)
MODE="inference"             

# Threshold tuning: MIMIC-Ext contains complex clinical questions
# 0.40 balances noise reduction while preserving relevant activations
CAM_THRESHOLD=0.40           
CAM_VERSION="gScoreCAM"       # Optimal for precise question-conditional localization

# ------------------------------------------------------------------------------
# 4. OUTPUT CONFIGURATION
# ------------------------------------------------------------------------------
# Generate visualizations for qualitative analysis
OUTPUT_FORMAT="image"        # 'image' for initial visual validation (observe model behavior)
STOP_AFTER=""

# ------------------------------------------------------------------------------
# 5. PROCESSING & HARDWARE SETTINGS
# ------------------------------------------------------------------------------
BATCH_SIZE=16
ENABLE_BODY_MASK="true"      # Enforce anatomical region constraints
ENABLE_SMART_PADDING="true"  # Apply anatomical boundary clamping
ENABLE_MULTI_BOXES="true"    # Enable multi-instance detection for questions like "Is there any...?"
MIN_BOX_AREA="0.002"         # Allow detection of small pathological findings